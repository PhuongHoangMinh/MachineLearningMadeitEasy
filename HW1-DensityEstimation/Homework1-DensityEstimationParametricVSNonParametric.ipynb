{
 "metadata": {
  "name": "",
  "signature": "sha256:988ebf747ef3cc9e95e2f6b5c5d0f710459cf87f4d00d2d6bcc84bca9064ed22"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#GOAL OF IRIS CLASSIFICATION\n",
      "Iris is a popular garden flower which has a wide variety of colors among several species. Mr. Fisher (1936) has collected the data of three typical species of Iris: Iris setosa, Iris virginica and Iris versicolor.\n",
      "The goal of this project is to classify the Iris data using parametric and nonparametric methods. Each data set consists of 50 samples from each of three species. Four features were measured from each sample: sepal length, sepal width, petal length, petal width.\n",
      "Furthermore, this project is a good chance to get familiar with basic computations in PYTHON.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#THEORITICAL BACKGROUND\n",
      "## Multivariable Normal Distribution\n",
      "The probability density function of the D-dimensional multivariate normal distribution is given by\n",
      "\n",
      "$ y = f(x,\\mu, \\sum) = \\frac{1}{\\sqrt{\\sum (2*\\pi)^d}} \\exp{\\frac{-1}{2} (x - \\mu)\\sum^{-1} (x-\\mu)^{\\prime}} $\n",
      "\n",
      "where $x$ and $\\mu$ are 1-by-d vectors and $\\sum$ is d-by-d symmetric definite matrix.\n",
      "\n",
      "The multivariate normal distribution is a generalization of the univariate normal to two or more variables. It is a distribution for random vectors of correlated variables, each element of which has a univariate normal distribution. In the simplest case, there is no correlation among variables, and elements of the vectors are independent univariate normal random variables.\n",
      "\n",
      "The multivariate normal distribution is parameterized with a mean vector, $\\mu$, and a covariance matrix, $\\sum$. These are analogous to the mean $\\mu$ and variance $\\sigma$ parameters of a univariate normal distribution. The diagonal elements of $\\sum$ contain the variances for each variable, while the off-diagonal elements of $\\sum$ contain the covariances between variables.\n",
      "\n",
      "The multivariate normal distribution is often used as a model for multivariate data, primarily because it is one of the few multivariate distributions that is tractable to work with.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Mixtures Models and EM using Gaussian distribution\n",
      "While the Gaussian distribution has some important analytical properties, it suffers from significant limitations when it comes to modelling real data sets. Consider the example shown in Figure 1. This is known as the \u2018Old Faithful\u2019 data set, which comprises 272 measurements of the eruption of the Old Faithful geyser at Yellowstone National Park in the USA. Each measurement comprises the duration of the eruption in minutes (horizontal axis) and the time in minutes to the next eruption (vertical axis). We see that the data set forms two dominant clumps, and that a simple Gaussian distribution is unable to capture this structure, whereas a linear superposition of two Gaussians gives a better characterization of the data set.\n",
      "\n",
      "Such superpositions, formed by taking linear combinations of more basic distributions such as Gaussians, can be formulated as probabilistic models known as mixture distributions (McLachlan and Basford, 1988; McLachlan and Peel, 2000). In Figure 2, we see that a linear combination of Gaussians can give rise to very complex densities. By using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.\n",
      "\n",
      "we therefore consider a superposition of K Gaussian densities of the form\n",
      "\n",
      "$p(x) = \\sum_{k=1}^{K}{\\pi_k \\mathcal{N}(x| \\mu_k, \\sum_k)}$\n",
      "\n",
      "where $\\pi_k$ are mixing coefficients\n",
      "\n",
      "$k : \\pi_k >= 0$    $\\sum_{k = 1}^{K}\\pi_k = 1$\n",
      "\n",
      "Determining parameters $\\mu, \\sum, \\pi$ using maximum log likelihood\n",
      "\n",
      "$\\ln p(X|\\pi,\\mu,\\sum) = \\sum_{n=1}^{N}{\\ln{\\sum_{k=1}^{K}{\\pi_k \\mathcal{N}(x| \\mu_k, \\sum_k)}}}$\n",
      "\n",
      "Use standard, iterative, numeric optimization methods or the expectation maximization algorithm (EM). Given a Gaussian mixture model, the goal is to maximize the likelihood function with respect to the parameters (comprising the means and covariances of the components and the mixing coefficients).\n",
      "   \n",
      "   Step 1: Initialize the means \u03bc_k, covariances \u03a3_k and mixing coefficients \u03c0_k, and evaluate the initial value of the log likelihood.\n",
      "   \n",
      "   Step 2: E step. Evaluate the responsibilities using the current parameter values\n",
      "\n",
      "$\\gamma(z_{nk}) = \\frac{\\pi_k\\mathcal{N}(x_n|\\mu_k, \\sum_k)}{\\sum_{j=1}^{K}\\pi_k \\mathcal{N}(x_n|\\mu_j, \\sum_j)} $\n",
      "\n",
      "   Step 3: M step. Re-estimate the parameters using the current responsibilities\n",
      "   \n",
      "   $\\mu_k^{new} = \\frac{1}{N_k}\\sum_{n=1}^{N}\\gamma(z_{nk})x_n$\n",
      "   \n",
      "   $\\sum_k^{new} = \\frac{1}{N_k}\\sum_{n=1}^{N}\\gamma(z_{nk})(x_n - \\mu_k^{new})(x_n - \\mu_k^{new})^T$\n",
      "   \n",
      "   $\\pi_k^{new} = \\frac{N_k}{N}$\n",
      "\n",
      "where\n",
      "\n",
      "   $N_k = \\sum_{n=1}^N \\gamma(z_{nk})$\n",
      "   \n",
      "   Step 4: Evaluate the log likelihood and check for convergence of either the parameters or the log likelihood. Return to step 2 if the convergence criterion is no satisfied "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Non parametric Method with Kernel density estimators and K-nearest neighbor\n",
      "\n",
      "Parametric approach is the use of probability distributions having specific functional forms governed by a small number of parameters whose values are to be determined from a data set. An important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data, which can result in poor predictive performance. For instance, if the process that generates the data is multimodal, then this aspect of the distribution can never be captured by a Gaussian, which is necessarily unimodal. \n",
      "\n",
      "An alternative approach is given by nonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set. Such models still contain parameters, but these control the model complexity rather than the form of the distribution. Some typical nonparametric methods are Kernel density estimators and K-nearest neighbor.\n",
      "\n",
      "The kernel density estimator will suffer from one of the same problems that the histogram method suffered from, namely the presence of artificial discontinuities, in this case at the boundaries of the cubes. To obtain a smoother density model if we choose a smoother kernel function, and a common choice is the Gaussian, which gives rise to the following kernel density model.\n",
      "\n",
      "$p(x) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{(2\\pi h^2)^(1/2)} \\exp{-\\frac{||x-x_n||^2}{2h^2}}$\n",
      "\n",
      "where h represents the standard deviation of the Gaussian components. Thus our density model is obtained by placing a Gaussian over each data point and then adding up the contributions over the whole data set, and then dividing by N so that the density is correctly normalized. In Figure 3, we apply the model to the data set used earlier to demonstrate the histogram technique. We see that, as expected, the parameter h plays the role of a smoothing parameter, and there is a trade-off between sensitivity to noise at small h and over-smoothing at large h. Again, the optimization of h is a problem in model complexity, analogous to the choice of bin width in histogram density estimation, or the degree of the polynomial used in curve fitting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import multivariate_normal\n",
      "\n",
      "dataIris = pd.read_table('iris.dat', delim_whitespace = True, header = None)\n",
      "dataIris.head()\n",
      "\n",
      "arrayIrisData = dataIris.as_matrix()\n",
      "\n",
      "irisSetosa = arrayIrisData[0:50, 0:4]\n",
      "irisVerisColor = arrayIrisData[50:100, 0:4]\n",
      "irisVirginica  = arrayIrisData[100:150, 0:4]\n",
      "\n",
      "train_mats1 = [irisSetosa[10:50,:], irisVerisColor[10:50,:], irisVirginica[10:50,:]]\n",
      "test_mats1  = [irisSetosa[0:10,:],  irisVerisColor[0:10,:], irisVirginica[0:10,:]]\n",
      "\n",
      "train_mats2 = [np.concatenate((irisSetosa[0:10, :], irisSetosa[20:50,:]), axis = 0), \n",
      "               np.concatenate((irisVerisColor[0:10,:], irisVerisColor[20:50,:]), axis = 0), \n",
      "               np.concatenate((irisVirginica[0:10,:], irisVirginica[20:50,:]), axis = 0)]\n",
      "test_mats2  = [irisSetosa[10:20,:], irisVerisColor[10:20,:], irisVirginica[10:20,:]]\n",
      "\n",
      "train_mats3 = [np.concatenate((irisSetosa[0:20, :], irisSetosa[30:50,:]), axis = 0), \n",
      "               np.concatenate((irisVerisColor[0:20,:], irisVerisColor[30:50,:]), axis = 0), \n",
      "               np.concatenate((irisVirginica[0:20,:], irisVirginica[30:50,:]), axis = 0)]\n",
      "test_mats3  = [irisSetosa[20:30,:], irisVerisColor[20:30,:], irisVirginica[20:30,:]]\n",
      "\n",
      "train_mats4 = [np.concatenate((irisSetosa[0:30, :], irisSetosa[40:50,:]), axis = 0), \n",
      "               np.concatenate((irisVerisColor[0:30,:], irisVerisColor[40:50,:]), axis = 0), \n",
      "               np.concatenate((irisVirginica[0:30,:], irisVirginica[40:50,:]), axis = 0)]\n",
      "test_mats4  = [irisSetosa[30:40,:], irisVerisColor[30:40,:], irisVirginica[30:40,:]]\n",
      "\n",
      "train_mats5 = [irisSetosa[0:40,:], irisVerisColor[0:40,:], irisVirginica[0:40,:]]\n",
      "test_mats5  = [irisSetosa[40:50,:], irisVerisColor[40:50,:],irisVirginica[40:50,:]]\n",
      "\n",
      "#train_mats\n",
      "train_mats = [train_mats1, train_mats2, train_mats3, train_mats4, train_mats5]\n",
      "test_mats  = [test_mats1, test_mats2, test_mats3, test_mats4, test_mats5]\n",
      "\n",
      "irisSetosa.shape\n",
      "train_mats1[0].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Problem 1: Gaussian assumption\n",
      "#Gaussian assumption for Iris Data\n",
      "def classify_question1(trainData, testData):\n",
      "    u1 = np.mean(trainData[0], axis = 0)\n",
      "    c1 = np.cov(trainData[0].T)\n",
      "    u2 = np.mean(trainData[1], axis = 0)\n",
      "    c2 = np.cov(trainData[1].T)\n",
      "    u3 = np.mean(trainData[2], axis = 0)\n",
      "    c3 = np.cov(trainData[2].T)\n",
      "    for i in range(3):\n",
      "        test = testData[i]\n",
      "        c = [0, 0, 0]\n",
      "        for x in range(10):\n",
      "            p1 = multivariate_normal.pdf(test[x,:], u1,c1)\n",
      "            p2 = multivariate_normal.pdf(test[x,:], u2,c2)\n",
      "            p3 = multivariate_normal.pdf(test[x,:], u3,c3)\n",
      "            parray = np.array([p1, p2, p3])\n",
      "            c[np.argmax(parray)] += 1;\n",
      "        print(c)\n",
      "        \n",
      "def question1(train, test):\n",
      "    print(\"Confusion matrix\")\n",
      "    C1 = classify_question1(train[0], test[0])\n",
      "    print(\"----------------------------------\")\n",
      "    C2 = classify_question1(train[1], test[1])\n",
      "    print(\"----------------------------------\")\n",
      "    C3 = classify_question1(train[2], test[2])\n",
      "    print(\"----------------------------------\")\n",
      "    C4 = classify_question1(train[3], test[3])\n",
      "    print(\"----------------------------------\")\n",
      "    C5 = classify_question1(train[4], test[4])\n",
      "    \n",
      "question1(train_mats, test_mats)\n",
      "\n",
      "#Accuracy = 98%"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Problem 1: Maximum Likelihood classifier with a Gaussian function\n",
      "The confusion matrices are shown as below figures\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "Using 5-fold cross validation technique, it can classify IRIS data with accuracy over 90%."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Maximum Likelihood classifier with a mixture of K Gaussians (K=2)\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "The classification accuracy is much better than using a unimodal Gaussian distribution. We can conclude that using the mixture of two Gaussian functions applying to the IRIS data is better than using only one Gaussian function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Problem 3: Maximum Likelihood classifier using Gaussian KDE\n",
      "\n",
      "h = 0.5\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 8  | 2  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "\n",
      "h = 10\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 8  | 2  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Problem 2: Solving the iris classification problem using non-parametric method - kernel density estimation (KDE)\n",
      "# the hyperparameter - h- is determined by cross-validation method \n",
      "# we choose Gaussian kernel to estimate the density to prevent discontinuities (boundaries of cube). Therefore, we can obtain smoother\n",
      "# density model  \n",
      "import math\n",
      "\n",
      "\n",
      "#Naive approach - using loop to estimate distance in the kernel\n",
      "def get_prob_2_Naive(trainData, testData, h):\n",
      "    p = 0.0\n",
      "    for i in range(40):\n",
      "        distance = np.linalg.norm(testData-trainData[i,:])\n",
      "        p = p + 1.0/(np.sqrt(2*math.pi)*h*np.exp((distance*distance)*0.5/(h*h)))\n",
      "    p = p/40.0\n",
      "    return p\n",
      "\n",
      "def classify_question_2(trainMats, testMats, h):\n",
      "    for i in range(3):\n",
      "        test = testMats[i]\n",
      "        c = [0, 0, 0]\n",
      "        for j in range(10):\n",
      "            p1 = get_prob_2_Naive(trainMats[0], test[j, :], h)\n",
      "            p2 = get_prob_2_Naive(trainMats[1], test[j, :], h)\n",
      "            p3 = get_prob_2_Naive(trainMats[2], test[j, :], h)\n",
      "            parray = np.array([p1,p2, p3])\n",
      "            c[np.argmax(parray)] += 1\n",
      "        print(c)\n",
      "\n",
      "def question2(train, test, h):\n",
      "    print(\"Confusion matrix\")\n",
      "    C1 = classify_question_2(train[0], test[0], h)\n",
      "    print(\"----------------------------------\")\n",
      "    C2 = classify_question_2(train[1], test[1], h)\n",
      "    print(\"----------------------------------\")\n",
      "    C3 = classify_question_2(train[2], test[2], h)\n",
      "    print(\"----------------------------------\")\n",
      "    C4 = classify_question_2(train[3], test[3], h)\n",
      "    print(\"----------------------------------\")\n",
      "    C5 = classify_question_2(train[4], test[4], h)\n",
      "\n",
      "h = 0.5\n",
      "question2(train_mats, test_mats, h)\n",
      "\n",
      "irisSetosa.astype('float')\n",
      "irisVerisColor.astype('float')\n",
      "irisVirginica.astype('float')\n",
      "\n",
      "irisSetosa_New = np.ndarray(shape = irisSetosa.shape, dtype = float)\n",
      "irisVerisColor_New = np.ndarray(shape = irisVerisColor.shape, dtype = float)\n",
      "irisVirginica_New = np.ndarray(shape = irisVirginica.shape, dtype = float)\n",
      "\n",
      "#normalizing dataset to range [-1 1] by seeking the maximum and the minimum samples\n",
      "for c in range(4):\n",
      "    maxSetosa_c = np.amax(irisSetosa[:, c])\n",
      "    minSetosa_c = np.amin(irisSetosa[:, c])\n",
      "    d_Setosa = maxSetosa_c - minSetosa_c\n",
      "    print(d_Setosa)\n",
      "  \n",
      "    \n",
      "    maxVerisColor_c = np.amax(irisVerisColor[:, c])\n",
      "    minVerisColor_c = np.amin(irisVerisColor[:, c])\n",
      "    d_VerisColor = maxVerisColor_c - minVerisColor_c\n",
      "\n",
      "    \n",
      "    maxVir_c = np.amax(irisVirginica[:,c])\n",
      "    minVir_c = np.amin(irisVirginica[:,c])\n",
      "    d_Vir = maxVir_c - minVir_c\n",
      "    \n",
      "    #print(\"Setosa: \",  irisSetosa[0,0], \"minSetosa: \", minSetosa_c, \"difference: \", (irisSetosa[0,c] - minSetosa_c)/d_Setosa*2.0 - 1.0)\n",
      "    for r in range(50):\n",
      "        irisSetosa_New[r, c] = (irisSetosa[r, c] - minSetosa_c)*2.0 /d_Setosa-1.0\n",
      "        irisVerisColor_New[r,c] = (irisVerisColor[r,c] - minVerisColor_c)*2.0 /d_VerisColor- 1.0\n",
      "        irisVirginica_New[r,c]  = (irisVirginica[r,c]- minVir_c)*2.0/d_Vir - 1.0\n",
      "\n",
      "\n",
      "def SetupDataset(irisSetosa, irisVerisColor, irirsVirginica):\n",
      "    train_mats1 = [irisSetosa[10:50,:], irisVerisColor[10:50,:], irisVirginica[10:50,:]]\n",
      "    test_mats1  = [irisSetosa[0:10,:],  irisVerisColor[0:10,:], irisVirginica[0:10,:]]\n",
      "\n",
      "    train_mats2 = [np.concatenate((irisSetosa[0:10, :], irisSetosa[20:50,:]), axis = 0), \n",
      "                   np.concatenate((irisVerisColor[0:10,:], irisVerisColor[20:50,:]), axis = 0), \n",
      "                   np.concatenate((irisVirginica[0:10,:], irisVirginica[20:50,:]), axis = 0)]\n",
      "    test_mats2  = [irisSetosa[10:20,:], irisVerisColor[10:20,:], irisVirginica[10:20,:]]\n",
      "\n",
      "    train_mats3 = [np.concatenate((irisSetosa[0:20, :], irisSetosa[30:50,:]), axis = 0), \n",
      "                   np.concatenate((irisVerisColor[0:20,:], irisVerisColor[30:50,:]), axis = 0), \n",
      "                   np.concatenate((irisVirginica[0:20,:], irisVirginica[30:50,:]), axis = 0)]\n",
      "    test_mats3  = [irisSetosa[20:30,:], irisVerisColor[20:30,:], irisVirginica[20:30,:]]\n",
      "\n",
      "    train_mats4 = [np.concatenate((irisSetosa[0:30, :], irisSetosa[40:50,:]), axis = 0), \n",
      "                   np.concatenate((irisVerisColor[0:30,:], irisVerisColor[40:50,:]), axis = 0), \n",
      "                   np.concatenate((irisVirginica[0:30,:], irisVirginica[40:50,:]), axis = 0)]\n",
      "    test_mats4  = [irisSetosa[30:40,:], irisVerisColor[30:40,:], irisVirginica[30:40,:]]\n",
      "\n",
      "    train_mats5 = [irisSetosa[0:40,:], irisVerisColor[0:40,:], irisVirginica[0:40,:]]\n",
      "    test_mats5  = [irisSetosa[40:50,:], irisVerisColor[40:50,:],irisVirginica[40:50,:]]\n",
      "\n",
      "\n",
      "    train_mats_New = [train_mats1, train_mats2, train_mats3, train_mats4, train_mats5]\n",
      "    test_mats_New  = [test_mats1, test_mats2, test_mats3, test_mats4, test_mats5]\n",
      "    return train_mats_New, test_mats_New\n",
      "\n",
      "train_mats_New, test_mats_New = SetupDataset(irisSetosa_New, irisVerisColor_New, irisVirginica_New)\n",
      "\n",
      "h = 10\n",
      "question2(train_mats_New, test_mats_New, h)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#K-nearest neighbor classifier with K=1, 3, 5\n",
      "K = 1\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 8  | 2  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "K = 3\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 8  | 2  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 3: classifying iris data using k-nearest neighbor algorithm\n",
      "def Compute_Distance_Two_Loops(X, X_train):\n",
      "    \"\"\"\n",
      "    compute the distance between test point in X and each training point in X_train\n",
      "    Input: \n",
      "    X - a numpy array has shape [NxD] - N - number of samples & D- dimesion of vector\n",
      "    X_train - a numpy array has shape [MxD] - M - number of training samples\n",
      "    Output:\n",
      "    dists - a numpy array that has shape of [NxM], \n",
      "    an element dists[i,j] - distance between i-th test point and j-th training point\n",
      "    \"\"\"\n",
      "    num_test = X.shape[0]\n",
      "    num_train = X_train.shape[0]\n",
      "    dists = np.zeros((num_test, num_train))\n",
      "    \n",
      "    for i in range(num_test):\n",
      "        for j in range(num_train):\n",
      "            dists[i,j] = np.linalg.norm(X_train[j,:]- X[i,:])\n",
      "    return dists\n",
      "\n",
      "def Compute_Distance_One_Loop(X, X_train):\n",
      "    \"\"\"\n",
      "    using broadcast to compute distance between test points and training samples\n",
      "    \"\"\"\n",
      "    num_test = X.shape[0]\n",
      "    num_train = X_train.shape[0]\n",
      "    dists = np.zeros((num_test, num_train))\n",
      "    for i in range(num_test):\n",
      "        dists[i, :] = np.sqrt(np.sum(np.square(X_train - X[i, :]), axis = 1))\n",
      "    \n",
      "    return dists\n",
      "\n",
      "def Compute_Distance_No_Loop(X, X_train):\n",
      "    \"\"\"\n",
      "    compute the distance between test point X and each training samples\n",
      "    \"\"\"\n",
      "    num_test = X.shape[0]  # N testing points\n",
      "    num_train = X_train.shape[0] # M training samples\n",
      "    dists = np.zeros([num_test, num_train])\n",
      "    \n",
      "    test_NxM = np.array([np.sum(np.square(X) , axis = 1)]*num_train).transpose()\n",
      "    #print(\"test shape: \",test_NxM.shape)\n",
      "    train_NxM = np.array([np.sum(np.square(X_train), axis = 1)]*num_test)\n",
      "    #print(\"train_shape: \", train_NxM.shape)\n",
      "    dists = np.sqrt(X.dot(X_train.transpose())*(-2) + test_NxM  + train_NxM)\n",
      "    \n",
      "    return dists\n",
      "\n",
      "def predict_labels_KNN(dists, y_train, k =1):\n",
      "    '''\n",
      "    given a dists matrix N*M between N test points and M training samples, predict label\n",
      "    for each test point\n",
      "    \n",
      "    Input:\n",
      "    dists - N*M matrix\n",
      "    Output:\n",
      "    y_predict - a vector of length num_test where y[i] is the predicted label for i-th test point\n",
      "    \n",
      "    '''\n",
      "    \n",
      "    num_test = dists.shape[0]\n",
      "    y_predict = np.zeros(num_test)\n",
      "    \n",
      "    for i in range(num_test):\n",
      "        # a list of length k storing the labels of k -nearest neighbors to the ith test point\n",
      "        closest_y = []\n",
      "        indexs = np.argsort(dists[i,:])\n",
      "        for w in range(0, k):\n",
      "            closest_y.append(y_train[indexs[w]])\n",
      "        \n",
      "        closest_y_int = [int(i) for i in closest_y]\n",
      "        count = []\n",
      "        labels = set(y_train)\n",
      "        for label_value in range(len(labels)):\n",
      "            count.append(0)\n",
      "        \n",
      "        \n",
      "        for j in range(k):\n",
      "            count[closest_y_int[j] - 1] += 1\n",
      "            \n",
      "        y_predict[i] = np.argmax(count) #note : return 0 - label type 1, return 1 - label type 2\n",
      "    \n",
      "    y_out = [int(i) for i in y_predict]\n",
      "    \n",
      "    return y_out\n",
      "\n",
      "# Test K-NN with iris data\n",
      "def classify_question_3(trainMats, testMats, num_loop = 2):\n",
      "    train = np.concatenate((trainMats[0],trainMats[1], trainMats[2]),axis = 0)\n",
      "    print(train.shape)\n",
      "    for i in range(3):\n",
      "        test = testMats[i]\n",
      "        c = [0, 0, 0]\n",
      "        #c[np.argmax(parray)] += 1\n",
      "        \n",
      "        \n",
      "        y_train = np.zeros(120)\n",
      "        y_train[0:40]  = 1\n",
      "        y_train[40:80] = 2\n",
      "        y_train[80:120]= 3\n",
      "        \n",
      "        #k-NN with two loops\n",
      "        if (num_loop == 2):\n",
      "            dists = Compute_Distance_Two_Loops(test, train)\n",
      "        #k-NN with one loops\n",
      "        elif (num_loop == 1):\n",
      "            dists = Compute_Distance_One_Loop(test, train)\n",
      "        #k-NN with no loop\n",
      "        else:\n",
      "            dists = Compute_Distance_No_Loop(test,train)\n",
      "            \n",
      "        \n",
      "        y_prec = predict_labels_KNN(dists, y_train, k =15)\n",
      "        for label in range(len(y_prec)):\n",
      "            c[y_prec[label]] += 1\n",
      "\n",
      "        \n",
      "        print(c)\n",
      "\n",
      "def question3(train, test, num_Loop = 2):\n",
      "    print(\"Confusion matrix\")\n",
      "    C1 = classify_question_3(train[0], test[0], num_Loop)\n",
      "    print(\"----------------------------------\")\n",
      "    C2 = classify_question_3(train[1], test[1], num_Loop)\n",
      "    print(\"----------------------------------\")\n",
      "    C3 = classify_question_3(train[2], test[2], num_Loop)\n",
      "    print(\"----------------------------------\")\n",
      "    C4 = classify_question_3(train[3], test[3], num_Loop)\n",
      "    print(\"----------------------------------\")\n",
      "    C5 = classify_question_3(train[4], test[4], num_Loop)\n",
      "\n",
      "question3(train_mats, test_mats, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Conclusion\n",
      "In this project, we utilize two approaches : parametric and non parametric to solve iris classification problem. In the parametric method, an assumption is made for the distribution of the data. Then by maximizing the likelihood, parameters are calculated. However, the parametric methods may turn out to be inappropriate for some applications. The nonparametric approaches forms the distribution depends on the size of the data set. Such models still contain parameters to control the model complexity rather than the form of the distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}