{
 "metadata": {
  "name": "",
  "signature": "sha256:20c4cd598dd77040dc079857bd185c1e2367a0c63ef7d17d19ebb5e30a9da6c6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Iris classification using Linear Classifications (Frequentist and Bayesian)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. Goal of Iris classification problem\n",
      "Iris is a popular garden flower which has a wide variety of colors among several species. Mr. Fisher (1936) has collected the data of three typical species of Iris: Iris setosa, Iris virginica and Iris versicolor. Each data set consists of 50 samples from each of three species. Four features were measured from each sample: sepal length, sepal width, petal length, petal width.\n",
      "\n",
      "\n",
      "The goal of this project is to use linear models to classify the Iris data. Alternatively, it means that taking a sample data in Iris dataset and to assign it to one of three discrete classes. We have several approaches to the classification problem. Firstly, the simplest involves constructing a discriminant function that directly assigns each vector x to a specific class. We also use three different methods here including 2-class classifier by one-versus-one combination, 2-class classifier by one-versus-the-rest combination, and 3-class classifier. Secondly, another approach is using the Fisher\u2019s discriminant function to transform the original data set to a new one that give out some benefits for classification. Finally, the method of Bayesian classifier using softmax function is also presented in this report.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2. Theoretical Background\n",
      "###2.1 Discriminant function\n",
      "A discriminant is a function that takes an input vector $x$ and assigns it to one of $K$ classes, denoted $C_k$. The simplest representation of a linear discriminant function is obtained by taking a linear function of the input vector so that\n",
      "\n",
      "$ y(x) = W^TX+ W_0$\n",
      "\n",
      "where $W$ is called a weight vector, and $w_0$ is a bias\n",
      "\n",
      "Consider a general classification problem with K classes, with a 1-of-K binary coding scheme for the target vector t. Each class $C_k$ is described by its own linear model so that \n",
      "\n",
      "$ y_k(x) = W_k^TX+ W_0$\n",
      "\n",
      "where $ k = 1\\cdots K$\n",
      "\n",
      "We can conveniently group these together using vector notation so that\n",
      "$ y(x) = \\check{W}^T\\check{x}$\n",
      "\n",
      "where $\\check{W}$ is a matrix whose $k^th$ column comprises the $D+1$ dimensional vector $\\check{w_k} = (w_{k0}, w_k^T)^T$  and $\\check{x}^T$ is the corresponding augmented input vector $(1, x^T)^T$\n",
      "\n",
      "Consider a training data set ${x_n, t_n}$ where $n = 1\\cdots N$ and define a matrix T whose $n^th$ row is the vector $t_n^T$ with a matrix $\\check{X}$ whose n row is $x_n^T$. The sum-of-squares error function can then be written as \n",
      "\n",
      "$E_D(\\check{W}) = \\frac{1}{1} Tr{(\\check{X}\\check{W} - T)^T(\\check{W}\\check{W} - T)}$\n",
      "\n",
      "Minimizing the error function with respect to $\\check{W}$.\n",
      "\n",
      "Setting the derivative with respect to $\\check{W}$ to zero, and rearranging, we obtain the solution for $\\check{W}$ in the form\n",
      "\n",
      "$\\check{W} = (\\check{X^T}\\check{X})^-1\\check{X^T} T$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2.2 Probabilistic generative models\n",
      "The probabilistic generative models is an approach that we model the class-conditional densities $p(x|\\mathcal{C_k})$ and the class priors $p(\\mathcal{C_k})$, and then use these to compute posterior probabilities $p(\\mathcal{C_k}|x)$ through Bayes' theorem\n",
      "\n",
      "The posterior probability for class $C_k$ can be written as\n",
      "\n",
      "$p(\\mathcal{C_k}|x) = \\frac{p(x|\\mathcal{C_k})p(\\mathcal{C_k})}{\\sum_j p(x|\\mathcal{C_j}) p(\\mathcal{C_j})} = \\frac{\\exp(a_k)}{\\sum_j\\exp(a_j)}$\n",
      "\n",
      "which is known as the normalized exponential and can be regarded as a multiclass generalizaation of the logistic sigmoid. Here the quantities $a_k$ can be defined by \n",
      "\n",
      "$a_k = \\ln p(x|\\mathcal{C_k}) p(\\mathcal{C_k})$\n",
      "\n",
      "Let us assume that the class-conditional densities are Gaussian and then explore the resulting form for the posterior probabilities. To start with , we shall assume that all classes share the same covariance matrix $\\sum$. \n",
      "For the case of K classes we have\n",
      "$a_k(x) = w_k^Tx + w_{k0}$\n",
      "\n",
      "where we have defined\n",
      "\n",
      "$w_k = \\sum^{-1}\\mu_k$\n",
      "\n",
      "$\\mathcal{w_{k0}} = \\frac{-1}{2}\\mu_k^T\\sum^{-1}\\mu_k + \\ln{p(\\mathcal{C_k})} $\n",
      "\n",
      "We can easily calculate the $\\mu_k$, which is the mean all the input vectors $x_n$ assigned to class $C_k$. $p(C_k)$ is given by the fraction of the number of data points in class $C_k$ and the total number of data points.\n",
      "Using the standard result for the maximum likelihood solution for a Gaussian distribution, we see that $\\sum = S$ which represents a weighted average of the covariance matrices associated with each of the two classes separately.\n",
      "\n",
      "$S = \\frac{N_1}{N}S_1 + \\frac{N_2}{N}S_2$\n",
      "\n",
      "$S_1 = \\frac{1}{N_1}\\sum{(x_n - \\mu_1)(x_n - \\mu_1)^T}$\n",
      "\n",
      "$S_2 = \\frac{1}{N_2}\\sum{(x_n - \\mu_2)(x_n - \\mu_2)^T}$\n",
      "\n",
      "The result is easily extended to the K class problem to obtain the corresponding maximum likelihood solution for the parameters in which each class-conditional density is Gaussian with a shared covariance matrix."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Implementation, Result & Discussion"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Implementation\n",
      "#Data preparation\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#Iris classification using maximum likelihood with Gaussian assumption\n",
      "#Cross-validation\n",
      "#  To classify the data set, firstly we use 5-fold cross validation method to \n",
      "#  divide each data set into two parts. In details, with each 50 samples of each class,\n",
      "#  we use 40 samples for training and 10 samples for testing. We will repeat this process 5 times\n",
      "#  with different training set and testing set.\n",
      "dataIris = pd.read_table('iris.dat', delim_whitespace = True, header = None)\n",
      "dataIris.head()\n",
      "\n",
      "\n",
      "arrayIrisData = dataIris.as_matrix()\n",
      "\n",
      "# Sepal length & width, petal length & width  X - shape - 1*4   - dataframe 150*4- 50samples/ type \n",
      "# 5-fold cross validation\n",
      "# 1-fold -testing\n",
      "# (n-1) fold - training \n",
      "irisSetosa = arrayIrisData[0:50, 0:4]\n",
      "irisVerisColor = arrayIrisData[50:100, 0:4]\n",
      "irisVirginica  = arrayIrisData[100:150, 0:4]\n",
      "\n",
      "train_mats1 = [irisSetosa[10:50,:], irisVerisColor[10:50,:], irisVirginica[10:50,:]]\n",
      "test_mats1  = [irisSetosa[0:10,:],  irisVerisColor[0:10,:], irisVirginica[0:10,:]]\n",
      "\n",
      "train_mats2 = [np.concatenate((irisSetosa[0:10, :], irisSetosa[20:50,:]), axis = 0), \n",
      "               np.concatenate((irisVerisColor[0:10,:], irisVerisColor[20:50,:]), axis = 0), \n",
      "               np.concatenate((irisVirginica[0:10,:], irisVirginica[20:50,:]), axis = 0)]\n",
      "test_mats2  = [irisSetosa[10:20,:], irisVerisColor[10:20,:], irisVirginica[10:20,:]]\n",
      "\n",
      "train_mats3 = [np.concatenate((irisSetosa[0:20, :], irisSetosa[30:50,:]), axis = 0), \n",
      "               np.concatenate((irisVerisColor[0:20,:], irisVerisColor[30:50,:]), axis = 0), \n",
      "               np.concatenate((irisVirginica[0:20,:], irisVirginica[30:50,:]), axis = 0)]\n",
      "test_mats3  = [irisSetosa[20:30,:], irisVerisColor[20:30,:], irisVirginica[20:30,:]]\n",
      "\n",
      "train_mats4 = [np.concatenate((irisSetosa[0:30, :], irisSetosa[40:50,:]), axis = 0), \n",
      "               np.concatenate((irisVerisColor[0:30,:], irisVerisColor[40:50,:]), axis = 0), \n",
      "               np.concatenate((irisVirginica[0:30,:], irisVirginica[40:50,:]), axis = 0)]\n",
      "test_mats4  = [irisSetosa[30:40,:], irisVerisColor[30:40,:], irisVirginica[30:40,:]]\n",
      "\n",
      "train_mats5 = [irisSetosa[0:40,:], irisVerisColor[0:40,:], irisVirginica[0:40,:]]\n",
      "test_mats5  = [irisSetosa[40:50,:], irisVerisColor[40:50,:],irisVirginica[40:50,:]]\n",
      "\n",
      "#train_mats\n",
      "train_mats = [train_mats1, train_mats2, train_mats3, train_mats4, train_mats5]\n",
      "test_mats  = [test_mats1, test_mats2, test_mats3, test_mats4, test_mats5]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "File b'iris.dat' does not exist",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-36ca3dc67a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#  we use 40 samples for training and 10 samples for testing. We will repeat this process 5 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#  with different training set and testing set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdataIris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdataIris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/theadmin/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/theadmin/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/theadmin/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/theadmin/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/theadmin/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/theadmin/.local/lib/python3.5/site-packages/pandas/parser.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/home/theadmin/.local/lib/python3.5/site-packages/pandas/parser.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mOSError\u001b[0m: File b'iris.dat' does not exist"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Question 1: one-versus-one using 2-class discriminant function\n",
      "#         y(x) = W.T*X +W0\n",
      "def classify_q1(train_mats, test_mats):\n",
      "    #add a column (all values = 1) to the current data\n",
      "    x_dummy = np.ones((40,1))\n",
      "    x1 = np.concatenate((train_mats[0], x_dummy),axis = 1)\n",
      "    print(\"train shape: \", train_mats[1].shape)\n",
      "    #print(\"test shape: \", x)\n",
      "    x2 = np.concatenate((train_mats[1], x_dummy),axis = 1)\n",
      "    x3 = np.concatenate((train_mats[2], x_dummy),axis = 1)\n",
      "    #print(x1.shape)\n",
      "    \n",
      "    \n",
      "    #initial value of the target value t\n",
      "    t = np.ones((80,1))\n",
      "    t[41:80] = 0\n",
      "    \n",
      "    #calculate the value w for class 1 versus 2\n",
      "    x12 = np.concatenate((x1, x2), axis = 0)\n",
      "    w12 = np.dot(np.dot(np.linalg.inv(np.dot(x12.T, x12)), x12.T), t)\n",
      "    #print(w12.shape)\n",
      "    \n",
      "    #calculate the valu w for class 1 versus 3\n",
      "    x13 = np.concatenate((x1, x3), axis  = 0)\n",
      "    w13 = np.dot(np.dot(np.linalg.inv(np.dot(x13.T, x13)), x13.T), t)\n",
      "    #print(x13.shape)\n",
      "    \n",
      "    #calculate the value w for class 2 versus 3\n",
      "    x23 = np.concatenate((x2, x3), axis = 0)\n",
      "    w23 = np.dot(np.dot(np.linalg.inv(np.dot(x23.T, x23)), x23.T), t)\n",
      "    #print(x23.shape)\n",
      "    \n",
      "    x_dummy_test = np.ones((10,1))\n",
      "    for i in range(3):\n",
      "        t = np.concatenate((test_mats[i], x_dummy_test), axis = 1)\n",
      "        c = [0, 0, 0]\n",
      "        #print(t.shape)\n",
      "        \n",
      "        #using major voting\n",
      "        for j in range(10):\n",
      "            p = [0, 0, 0]\n",
      "            \n",
      "            y12 = np.dot(w12.T, t[j,:])\n",
      "            if y12 > 0.5:\n",
      "                p[0] = p[0] + 1\n",
      "            else:\n",
      "                p[1] = p[1] + 1\n",
      "                \n",
      "            y13 = np.dot(w13.T, t[j,:])\n",
      "            if y13> 0.5:\n",
      "                p[0] = p[0] + 1\n",
      "            else:\n",
      "                p[2] = p[2] + 1\n",
      "                \n",
      "            y23 = np.dot(w23.T, t[j,:])\n",
      "            if y23 > 0.5:\n",
      "                p[1] = p[1] + 1\n",
      "            else:\n",
      "                p[2] = p[2] + 1\n",
      "                \n",
      "            c[np.argmax(p)] += 1;\n",
      "        print(c)\n",
      "        \n",
      "def question1(train, test):\n",
      "    print(\"Confusion matrix\")\n",
      "    C1 = classify_q1(train[0], test[0])\n",
      "    print(\"----------------------------------\")\n",
      "    C2 = classify_q1(train[1], test[1])\n",
      "    print(\"----------------------------------\")\n",
      "    C3 = classify_q1(train[2], test[2])\n",
      "    print(\"----------------------------------\")\n",
      "    C4 = classify_q1(train[3], test[3])\n",
      "    print(\"----------------------------------\")\n",
      "    C5 = classify_q1(train[4], test[4])\n",
      "\n",
      "question1(train_mats, test_mats)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2-class discriminant function classifiers using one-versus-one combination\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "\n",
      "The accuracy rate is 98.67%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Question 2: one-versus-rest using 2-class discriminant function\n",
      "def classify_q2(train_mats, test_mats):\n",
      "    #add a column (all values = 1) to the current data\n",
      "    x_dummy = np.ones((40,1))\n",
      "    x1 = np.concatenate((train_mats[0], x_dummy),axis = 1)\n",
      "    x2 = np.concatenate((train_mats[1], x_dummy),axis = 1)\n",
      "    x3 = np.concatenate((train_mats[2], x_dummy),axis = 1)\n",
      "    \n",
      "    #initial value of the target value t\n",
      "    t = np.zeros((120,1))\n",
      "    t[0:40] = 1\n",
      "    \n",
      "    #calculate w for classifying class 1 or not class 1\n",
      "    x11 = np.concatenate((x1, x2, x3), axis = 0)\n",
      "    w11 = np.dot(np.dot(np.linalg.inv(np.dot(x11.T, x11)), x11.T), t)\n",
      "    \n",
      "    t = np.zeros((120,1))\n",
      "    t[41:80] = 1\n",
      "    \n",
      "    #calculate w for classifying class 2 or not class 2\n",
      "    x22 = x11.copy()\n",
      "    w22 = np.dot(np.dot(np.linalg.inv(np.dot(x11.T, x11)), x11.T), t)\n",
      "    \n",
      "    x_dummy_test = np.ones((10,1))\n",
      "    for i in range(3):\n",
      "        t = np.concatenate((test_mats[i], x_dummy_test), axis = 1)\n",
      "        c = [0, 0, 0]\n",
      "        #print(t.shape)\n",
      "        \n",
      "        #using major voting\n",
      "        for j in range(10):\n",
      "            y1 = np.dot(w11.T, t[j, :])\n",
      "            \n",
      "            if y1> 0.5: \n",
      "                c[0] = c[0] + 1\n",
      "            else:\n",
      "                y2 = np.dot(w22.T, t[j,:])\n",
      "                if y2 > 0.5:\n",
      "                    c[1] = c[1] + 1\n",
      "                else:\n",
      "                    c[2] = c[2] + 1\n",
      "        print(c)\n",
      "        \n",
      "def question2(train, test):\n",
      "    print(\"Confusion matrix\")\n",
      "    C1 = classify_q2(train[0], test[0])\n",
      "    print(\"----------------------------------\")\n",
      "    C2 = classify_q2(train[1], test[1])\n",
      "    print(\"----------------------------------\")\n",
      "    C3 = classify_q2(train[2], test[2])\n",
      "    print(\"----------------------------------\")\n",
      "    C4 = classify_q2(train[3], test[3])\n",
      "    print(\"----------------------------------\")\n",
      "    C5 = classify_q2(train[4], test[4])\n",
      "    \n",
      "question2(train_mats, test_mats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2-class discriminant function classifiers using one-versus-rest combination\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 4  | 6  |\n",
      "| C3 |    | 4  | 6  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 6  | 4  |\n",
      "| C3 |    | 2  | 8  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 4  | 6  |\n",
      "| C3 |    | 2  | 8  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 6  | 4  |\n",
      "| C3 |    | 3  | 7  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 4  | 6  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "the accuracy rate is 74.67%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Question 3: Using multi class discriminant function\n",
      "def classify_q3(train_mats, test_mats):\n",
      "    #add a column (all values = 1) to the current data\n",
      "    x_dummy = np.ones((40,1))\n",
      "    x1 = np.concatenate((train_mats[0], x_dummy),axis = 1)\n",
      "    x2 = np.concatenate((train_mats[1], x_dummy),axis = 1)\n",
      "    x3 = np.concatenate((train_mats[2], x_dummy),axis = 1)\n",
      "    \n",
      "    #initial value of the target value t\n",
      "    t = np.zeros((120,3))\n",
      "    t[0:40, :] = [1, 0, 0]\n",
      "    t[40:80, :]= [0, 1, 0]\n",
      "    t[80:120,:]= [0, 0, 1]\n",
      "    \n",
      "    #calculate w\n",
      "    x = np.concatenate((x1, x2, x3), axis = 0)\n",
      "    w = np.dot(np.dot(np.linalg.inv(np.dot(x.T, x)), x.T), t)\n",
      "    \n",
      "    x_dummy_test = np.ones((10,1))\n",
      "    for i in range(3):\n",
      "        t = np.concatenate((test_mats[i], x_dummy_test), axis = 1)\n",
      "        c = [0, 0, 0]\n",
      "        #print(t.shape)\n",
      "        \n",
      "        #using major voting\n",
      "        for j in range(10):\n",
      "            p = np.dot(w.T, t[j, :])\n",
      "            c[np.argmax(p)] += 1\n",
      "        print(c)\n",
      "\n",
      "def question3(train, test):\n",
      "    print(\"Confusion matrix\")\n",
      "    C1 = classify_q3(train[0], test[0])\n",
      "    print(\"----------------------------------\")\n",
      "    C2 = classify_q3(train[1], test[1])\n",
      "    print(\"----------------------------------\")\n",
      "    C3 = classify_q3(train[2], test[2])\n",
      "    print(\"----------------------------------\")\n",
      "    C4 = classify_q3(train[3], test[3])\n",
      "    print(\"----------------------------------\")\n",
      "    C5 = classify_q3(train[4], test[4])\n",
      "\n",
      "question3(train_mats, test_mats)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3-class discrimimant function classifier\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 6  | 4  |\n",
      "| C3 |    | 3  | 7  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 6  | 4  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 6  | 4  |\n",
      "| C3 |    | 2  | 8  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 6  | 4  |\n",
      "| C3 |    | 3  | 7  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 8  | 2  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "The accuracy rate is 81.33%\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3-class Bayesian classifier using softmax function\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 9  | 1  |\n",
      "| C3 |    | 1  | 9  |\n",
      "\n",
      "|    | C1 | C2 | C3 |\n",
      "|----|----|----|----|\n",
      "| C1 | 10 |    |    |\n",
      "| C2 |    | 10 |    |\n",
      "| C3 |    |    | 10 |\n",
      "\n",
      "The accuracy rate is 98%"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "| Method                                                                        | No of wrong classification | Performance |\n",
      "|-------------------------------------------------------------------------------|----------------------------|-------------|\n",
      "| Maximum likelihood classifier - Gaussian assumption                           | 4/150                      | 97.33%      |\n",
      "| Maximum likelihood classifier- a mixture of K Gaussians (K=2)                 | 2/150                      | 98.67%      |\n",
      "| Maximum likelihood classifier - Gaussian KDE                                  | 6/150                      | 96%         |\n",
      "| K-nearest neighbor                                                            | 6/150                      | 96%         |\n",
      "| several 2-class discriminant functions using one-vs-one combination           | 2/150                      | 98.67%      |\n",
      "| several 2-class discriminant function using one-vs-rest combination           | 38/150                     | 74.67%      |\n",
      "| 3-class discriminant function classifier                                      | 28/150                     | 81.33%      |\n",
      "| 3-class Bayesian classifier using softmax function (generative probabilistic) | 3/150                      | 98%         |"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}